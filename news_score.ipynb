{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "429d0acc-92bd-44f5-8734-30d48cde210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from google.cloud import storage\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import csv\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d41a61-bdc4-44db-903a-ab1aa335b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/mnt/disks/data/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"  # Optional: if you're only working locally\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/mnt/disks/data/transformers_cache\"\n",
    "os.environ[\"TMPDIR\"] = \"/mnt/disks/data/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f905d6fb-d1b8-4858-98dc-0b08bf1732a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "# Detect number of GPUs\n",
    "gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {gpus}\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8e0baa-df29-4673-a4aa-083e1b278c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "BUCKET_NAME = \"diss_market_data\"\n",
    "MODEL_PREFIX = \"deberta_news/\"\n",
    "MODEL_LOCAL_DIR = \"./deberta_news\"\n",
    "DATASET = \"danidanou/Bloomberg_Financial_News\"\n",
    "OUTPUT_CSV_PATH = \"./Data/news_sentiment_scores.csv\"\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_STRIDE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c744c95-9312-4395-aacc-2db8d80b14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- GCS Download Helpers ---------------------\n",
    "def download_from_gcs(bucket_name, prefix, local_dir):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('/'):  # Skip folders\n",
    "            continue\n",
    "        rel_path = os.path.relpath(blob.name, prefix)\n",
    "        local_path = os.path.join(local_dir, rel_path)\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        blob.download_to_filename(local_path)\n",
    "\n",
    "# Download model if not present\n",
    "if not os.path.exists(os.path.join(MODEL_LOCAL_DIR, \"pytorch_model.bin\")):\n",
    "    download_from_gcs(BUCKET_NAME, MODEL_PREFIX, MODEL_LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9129d0-64cf-4bd2-a056-fb44b2e20bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------ Load Model -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_LOCAL_DIR)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_LOCAL_DIR).to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "099874ab-0e3d-45c4-b8d8-713672a7ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_chunks(text, max_len=512, stride=128):\n",
    "    tokens = tokenizer(text, truncation=False, padding=False, return_tensors='pt')['input_ids'][0]\n",
    "    chunks = [tokens[i:i+max_len] for i in range(0, len(tokens), max_len - stride)]\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141cc294-b15e-4890-8271-bceea03cea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_predict(chunks):\n",
    "    inputs = tokenizer(chunks, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}  # âœ… ensure proper device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "    labels = probs.argmax(axis=1)\n",
    "    label_map = ['negative', 'neutral', 'positive']\n",
    "    results = []\n",
    "    for i, score in enumerate(probs):\n",
    "        label = label_map[labels[i]]\n",
    "        scaled_score = max(score) * (1 if label == 'positive' else -1 if label == 'negative' else 0)\n",
    "        results.append((scaled_score, label))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e239bca-c13e-4a3d-81ac-5a8b79046be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0d6caf9-2634-4185-b144-cda24466ccfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Headline', 'Journalists', 'Date', 'Link', 'Article'],\n",
       "    num_rows: 446762\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ef8b366-ce58-4390-8b0f-28f2bec005c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_id(row):\n",
    "    return hashlib.md5((row['Headline']).encode('utf-8')).hexdigest()\n",
    "\n",
    "# Load existing processed records\n",
    "if os.path.exists(OUTPUT_CSV_PATH):\n",
    "    done_df = pd.read_csv(OUTPUT_CSV_PATH)\n",
    "    done_ids = set(done_df.apply(lambda row: compute_id(row), axis=1))\n",
    "else:\n",
    "    done_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f05a20f-c9ac-4cea-92fe-fa2877f2b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_record(record):\n",
    "    date = record.get('Date', '')\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    headline = record.get('Headline', '').strip()\n",
    "    text = record.get('Article', '').strip()\n",
    "    \n",
    "    record_id = hashlib.md5((headline).encode('utf-8')).hexdigest()\n",
    "    if record_id in done_ids:\n",
    "        return []\n",
    "    chunks = get_section_chunks(text)\n",
    "    if not text or not chunks:\n",
    "        return []\n",
    "\n",
    "    sentiments = batched_predict(chunks)\n",
    "    scores = [s for s, _ in sentiments]\n",
    "    labels = [l for _, l in sentiments]\n",
    "\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    majority_label = max(set(labels), key=labels.count)\n",
    "\n",
    "    results.append({\n",
    "        'Article': headline,\n",
    "        'Date': date,\n",
    "        'Sentiment Score': avg_score,\n",
    "        'Sentiment Label': majority_label\n",
    "    })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d960f94-25ce-418a-82d1-3ec4ea8b2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------ Load & Stream Dataset -------------------------\n",
    "# splits = [\"train\"]\n",
    "# all_results = []\n",
    "\n",
    "# for split in splits:\n",
    "#     ds = dataset[split]\n",
    "#     for record in tqdm(ds, desc=f\"Processing {split}\"):\n",
    "#         results = process_record(record)\n",
    "#         all_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11cdc7c-9241-4cb9-ad32-c1fcbeb8202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------ Save Results ----------------------------------\n",
    "# df = pd.DataFrame(all_results)\n",
    "# df.to_csv(OUTPUT_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6586d280-2e11-4e69-b1a0-c6082e67332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create CSV and write header once\n",
    "# with open(OUTPUT_CSV_PATH, mode='w', newline='', encoding='utf-8') as f:\n",
    "#     writer = csv.DictWriter(f, fieldnames=[\n",
    "#         'Article', 'Date', 'Sentiment Score', 'Sentiment Label'\n",
    "#     ])\n",
    "#     writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24d1a224-8647-4cb7-b94e-6d63a3fef7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 446762/446762 [4:27:12<00:00, 27.87it/s]\n"
     ]
    }
   ],
   "source": [
    "buffer = []\n",
    "splits = [\"train\"]\n",
    "for split in splits:\n",
    "    ds = dataset[split]\n",
    "    for record in tqdm(ds, desc=f\"Processing {split}\"):\n",
    "        results = process_record(record)\n",
    "        if results:\n",
    "            buffer.extend(results)\n",
    "        if len(buffer) >= 1000:\n",
    "            with open(OUTPUT_CSV_PATH, mode='a', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=[\n",
    "                    'Article', 'Date', 'Sentiment Score', 'Sentiment Label'\n",
    "                ])\n",
    "                writer.writerows(buffer)\n",
    "            buffer = []  # clear buffer\n",
    "\n",
    "# Final flush\n",
    "if buffer:\n",
    "    with open(OUTPUT_CSV_PATH, mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            'Article', 'Date', 'Sentiment Score', 'Sentiment Label'\n",
    "        ])\n",
    "        writer.writerows(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018b228-6d46-4e42-93a0-8cc61b2b9932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fusionnet)",
   "language": "python",
   "name": "fusionnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
