{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0194a5f5-cd37-4fbe-8d50-e35fc60b2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b89182-4080-4211-8ce1-16b91a760164",
   "metadata": {},
   "source": [
    "### Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "120091ba-6f7d-4226-ae1f-c293957bb6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_snp = \"Data/SPX_Weekly_06-14.csv\"\n",
    "path_10k = \"Data/weekly_10k_weighted.csv\"\n",
    "path_10q = \"Data/weekly_10q_weighted.csv\"\n",
    "path_8k = \"Data/weekly_8k_weighted.csv\"\n",
    "path_news = \"Data/weekly_smoothed_news.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "aaf8b9fb-e814-4e1e-bd8e-f6c57e2f69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weekly datasets\n",
    "news = pd.read_csv(path_news, parse_dates=['Date']).rename(columns={'Date':'Week'})\n",
    "tenk = pd.read_csv(path_10k, parse_dates=['Week'])\n",
    "tenq = pd.read_csv(path_10q, parse_dates=['Week'])\n",
    "eightk = pd.read_csv(path_8k, parse_dates=['Week'])\n",
    "sp500 = pd.read_csv(path_snp, parse_dates=['Date']).rename(columns={'Date':'Week'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "85037b4a-fc0a-4150-8a16-9a0730446e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>mean_sent</th>\n",
       "      <th>count</th>\n",
       "      <th>total_wscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2013-11-17</td>\n",
       "      <td>-0.296086</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.001568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2013-11-24</td>\n",
       "      <td>-0.338836</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.001241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2013-12-01</td>\n",
       "      <td>-0.135738</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.001311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2013-12-08</td>\n",
       "      <td>-0.121426</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2013-12-22</td>\n",
       "      <td>-0.769444</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.005239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Week  mean_sent  count  total_wscore\n",
       "234 2013-11-17  -0.296086      2     -0.001568\n",
       "235 2013-11-24  -0.338836      2     -0.001241\n",
       "236 2013-12-01  -0.135738      4     -0.001311\n",
       "237 2013-12-08  -0.121426      1     -0.000175\n",
       "238 2013-12-22  -0.769444      3     -0.005239"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenk.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "48f09c9b-d824-4b3f-b5ba-ab83d37dcad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract just the date\n",
    "for df in [news, tenk, tenq, eightk]:\n",
    "    df['Week'] = df['Week'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "865ffd37-6cd8-41c4-9063-25c035e0aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [news, tenk, tenq, eightk, sp500]\n",
    "\n",
    "for df in dfs:\n",
    "    # ensure Week is a datetime64[ns]\n",
    "    df['Week'] = pd.to_datetime(df['Week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "1c1ceee7-4f22-45ee-906f-4d475626abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on Week\n",
    "df = news.merge(tenk, on='Week', how='inner') \\\n",
    "         .merge(tenq, on='Week', how='inner') \\\n",
    "         .merge(eightk, on='Week', how='inner') \\\n",
    "         .merge(sp500, on='Week', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "17fc281e-abd6-442f-b837-17ab43cb3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0_x', 'Unnamed: 0_y', 'log_return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17219aa6-fde2-4316-8c52-3e8a4824a52a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "c9182f96-7926-49e6-9558-cde88f079e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Week', 'mean_news_sentiment', 'std_news_sentiment', 'num_positive',\n",
       "       'num_negative', 'num_neutral', 'num_articles', 'low_coverage_week',\n",
       "       'smoothed_sentiment', 'mean_sent_x', 'count_x', 'total_wscore_x',\n",
       "       'mda_sent', 'risk_sent', 'mda_sent_weighted', 'risk_sent_weighted',\n",
       "       'count_10q', 'mda_smoothed', 'risk_smoothed', 'mda_smoothed_weighted',\n",
       "       'risk_smoothed_weighted', 'opt_vs_caut', 'opt_vs_caut_weighted',\n",
       "       'mean_sent_y', 'count_y', 'total_wscore_y', 'Open', 'High', 'Low',\n",
       "       'Close', 'Adj Close', 'Volume', 'Year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "ade3112e-5983-441b-bc0e-957d1f42d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'smoothed_sentiment': 'smoothed_news_sentiment',\n",
    "                  'num_articles': 'num_news_articles',\n",
    "                   'mean_sent_x': '10k_mean_sent',\n",
    "                   'count_x' : '10k_count', \n",
    "                   'total_wscore_x' : '10k_total_wscore',\n",
    "                  'mda_sent': '10q_mda_sent', \n",
    "                   'mda_sent_weighted' : '10q_mda_sent_weighted',\n",
    "                  'risk_sent': '10q_risk_sent', \n",
    "                   'risk_sent_weighted' : '10q_risk_sent_weighted',\n",
    "                   'mean_sent_y' : '8k_mean_sent', \n",
    "                   'count_y' : '8k_count',\n",
    "                   'total_wscore_y' : '8k_total_wscore'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b2bb7e8f-c34b-4554-8730-69923b30ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data/merged_dataset_weighted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7dd3b-3eef-45d9-aa12-aaad4a5d3b9b",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f0fca-0dff-48a8-a28b-77834d4570e4",
   "metadata": {},
   "source": [
    "### Using all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3ed028-10cb-48a7-9635-c52a0932efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f400b79-91af-42a5-8fbe-770423287478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load merged dataset\n",
    "df = pd.read_csv(\"Data/merged_dataset_weighted.csv\", parse_dates=[\"Week\"])\n",
    "df = df.sort_values(\"Week\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b18cbe-de2f-43de-b3eb-1da4cbebb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weekly return and target\n",
    "df[\"Return\"] = df[\"Close\"].pct_change()\n",
    "df[\"Target\"] = np.where(df[\"Return\"] > 0.01, 1,\n",
    "                np.where(df[\"Return\"] < -0.01, -1, 0))\n",
    "df = df.dropna(subset=[\"Return\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af36de36-976b-4810-9ef3-dc4487f21e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Week', 'mean_news_sentiment', 'std_news_sentiment', 'num_positive',\n",
       "       'num_negative', 'num_neutral', 'num_news_articles', 'low_coverage_week',\n",
       "       'smoothed_news_sentiment', '10k_mean_sent', '10k_count',\n",
       "       '10k_total_wscore', '10q_mda_sent', '10q_risk_sent',\n",
       "       '10q_mda_sent_weighted', '10q_risk_sent_weighted', 'count_10q',\n",
       "       'mda_smoothed', 'risk_smoothed', 'mda_smoothed_weighted',\n",
       "       'risk_smoothed_weighted', 'opt_vs_caut', 'opt_vs_caut_weighted',\n",
       "       '8k_mean_sent', '8k_count', '8k_total_wscore', 'Open', 'High', 'Low',\n",
       "       'Close', 'Adj Close', 'Volume', 'Year', 'Return', 'Target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff871fd7-8650-475a-9d43-5ac8d1ab6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns\n",
    "exclude = {\"Week\", \"Close\", \"Return\", \"Target\", 'Year', 'Adj Close', 'mean_news_sentiment', 'std_news_sentiment', '10q_mda_sent', '10q_risk_sent',\n",
    "       '10q_mda_sent_weighted', '10q_risk_sent_weighted' }\n",
    "feature_cols = [c for c in df.columns if c not in exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "493e6dae-5724-4efa-ba69-7fea0f3bb693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged features\n",
    "for lag in [1, 2]:\n",
    "    df[[f\"{col}_lag{lag}\" for col in feature_cols]] = df[feature_cols].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9743e9b6-9f59-453a-a854-80f038345d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f385de09-1333-47f2-a207-230551797cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split: 80/20\n",
    "split = int(len(df_model) * 0.8)\n",
    "train, test = df_model.iloc[:split], df_model.iloc[split:]\n",
    "X_train, y_train = train[feature_cols + [f\"{c}_lag1\" for c in feature_cols] + [f\"{c}_lag2\" for c in feature_cols]], train[\"Target\"]\n",
    "X_test, y_test   = test[feature_cols + [f\"{c}_lag1\" for c in feature_cols] + [f\"{c}_lag2\" for c in feature_cols]], test[\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd3989fc-8957-4092-9108-5cbf3df30e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit RF\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c33cb50b-665e-4207-b1a1-70aaa579df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11642f51-891c-4120-a93a-08be94475329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Report ===\n",
      "              precision    recall  f1-score    support\n",
      "-1             0.428571  0.300000  0.352941  10.000000\n",
      "0              0.500000  0.777778  0.608696  18.000000\n",
      "1              0.250000  0.090909  0.133333  11.000000\n",
      "accuracy       0.461538  0.461538  0.461538   0.461538\n",
      "macro avg      0.392857  0.389562  0.364990  39.000000\n",
      "weighted avg   0.411172  0.461538  0.409041  39.000000\n",
      "\n",
      "=== Summary Metrics ===\n",
      "   Accuracy  Matthews CC\n",
      "0  0.461538     0.103722\n"
     ]
    }
   ],
   "source": [
    "# Classification report to DataFrame and display\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "print(\"=== Classification Report ===\")\n",
    "print(report_df)\n",
    "\n",
    "# Create a summary metrics DataFrame and display\n",
    "summary_df = pd.DataFrame([{\"Accuracy\": acc, \"Matthews CC\": mcc}])\n",
    "print(\"\\n=== Summary Metrics ===\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028baba4-c98c-4a9f-84ae-a2154e38a29c",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "b9976199-4baf-4ca1-ad6c-aaa489207534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "adfb4003-1a59-4b27-8765-6d1acc094661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_positive',\n",
       " 'num_negative',\n",
       " 'num_neutral',\n",
       " 'num_news_articles',\n",
       " 'low_coverage_week',\n",
       " 'smoothed_news_sentiment',\n",
       " '10k_mean_sent',\n",
       " '10k_count',\n",
       " '10k_total_wscore',\n",
       " 'count_10q',\n",
       " 'mda_smoothed',\n",
       " 'risk_smoothed',\n",
       " 'mda_smoothed_weighted',\n",
       " 'risk_smoothed_weighted',\n",
       " 'opt_vs_caut',\n",
       " 'opt_vs_caut_weighted',\n",
       " '8k_mean_sent',\n",
       " '8k_count',\n",
       " '8k_total_wscore',\n",
       " 'Open',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Volume']"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "76de2542-0671-43dc-bd79-487345952348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = df[feature_cols + [f\"{c}_lag1\" for c in feature_cols] + [f\"{c}_lag2\" for c in feature_cols]]\n",
    "y = df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "49a433ab-04e4-4ae2-8ff4-5383693f6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = feature_cols + [f\"{c}_lag1\" for c in feature_cols] + [f\"{c}_lag2\" for c in feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "c4d350e3-1b45-4b6d-a7a0-df017acb6d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance per fold:\n",
      "\n",
      "Fold 1:\n",
      "  Train: {1: 0.40540540540540543, 0: 0.2972972972972973, -1: 0.2972972972972973}\n",
      "  Test:  {-1: 0.4, 1: 0.4, 0: 0.2}\n",
      "\n",
      "Fold 2:\n",
      "  Train: {1: 0.4027777777777778, -1: 0.3472222222222222, 0: 0.25}\n",
      "  Test:  {1: 0.4, 0: 0.4, -1: 0.2}\n",
      "\n",
      "Fold 3:\n",
      "  Train: {1: 0.40186915887850466, 0: 0.29906542056074764, -1: 0.29906542056074764}\n",
      "  Test:  {1: 0.45714285714285713, -1: 0.3142857142857143, 0: 0.22857142857142856}\n",
      "\n",
      "Fold 4:\n",
      "  Train: {1: 0.4154929577464789, -1: 0.3028169014084507, 0: 0.28169014084507044}\n",
      "  Test:  {0: 0.42857142857142855, 1: 0.3142857142857143, -1: 0.2571428571428571}\n",
      "\n",
      "Fold 5:\n",
      "  Train: {1: 0.3954802259887006, 0: 0.3107344632768362, -1: 0.2937853107344633}\n",
      "  Test:  {0: 0.4857142857142857, 1: 0.3142857142857143, -1: 0.2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. TimeSeriesSplit class balance diagnostics\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "print(\"Class balance per fold:\\n\")\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(\"  Train:\", y_train.value_counts(normalize=True).to_dict())\n",
    "    print(\"  Test: \", y_test.value_counts(normalize=True).to_dict())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b328fac-b3b0-400f-969d-3c79baeb7e61",
   "metadata": {},
   "source": [
    "##### Class balance is reasonably stable\n",
    "Across all five folds, “up” (1) weeks make up roughly 39–41 % of the train splits and 33–44 % of the test splits; “down” (–1) weeks hover around 31–32 % in-train and 23–44 % out-of-sample; “flat” weeks fill the remainder. In other words, no fold is so skewed that your model sees almost no examples of one class—but the splits do drift by ±10 % in the test set. That variation can itself add noise to your performance read-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "f1f126f2-d9ae-4b5c-bc19-927037f90494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>abs_corr_with_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count_10q_lag1</td>\n",
       "      <td>0.198807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opt_vs_caut_weighted_lag1</td>\n",
       "      <td>0.169777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8k_mean_sent_lag2</td>\n",
       "      <td>0.155822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10k_mean_sent_lag2</td>\n",
       "      <td>0.151561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8k_count_lag1</td>\n",
       "      <td>0.150599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10k_total_wscore</td>\n",
       "      <td>0.146635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10k_count</td>\n",
       "      <td>0.135514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8k_mean_sent_lag1</td>\n",
       "      <td>0.131702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>risk_smoothed_weighted_lag1</td>\n",
       "      <td>0.122583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>opt_vs_caut_weighted_lag2</td>\n",
       "      <td>0.122407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature  abs_corr_with_target\n",
       "0               count_10q_lag1              0.198807\n",
       "1    opt_vs_caut_weighted_lag1              0.169777\n",
       "2            8k_mean_sent_lag2              0.155822\n",
       "3           10k_mean_sent_lag2              0.151561\n",
       "4                8k_count_lag1              0.150599\n",
       "5             10k_total_wscore              0.146635\n",
       "6                    10k_count              0.135514\n",
       "7            8k_mean_sent_lag1              0.131702\n",
       "8  risk_smoothed_weighted_lag1              0.122583\n",
       "9    opt_vs_caut_weighted_lag2              0.122407"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Feature–target Pearson correlations\n",
    "corrs = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "corr_df = corrs.reset_index()\n",
    "corr_df.columns = ['feature', 'abs_corr_with_target']\n",
    "display(corr_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33caa0a5-6ff2-4573-8613-22a156052b7d",
   "metadata": {},
   "source": [
    "##### Sentiment–return correlations are near zero\n",
    "Even the best lagged sentiment feature ( sent_8k_mean_lag2, |ρ|≈0.14 ) explains only 2 % of weekly variance. The bulk of your sentiment and filing aggregates correlate with returns at |ρ|<0.08. That implies there’s no strong linear signal for the model to pick up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "10a034a6-c41a-462b-8b24-3f55dd1a5ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smoothed_news_sentiment</td>\n",
       "      <td>0.023619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opt_vs_caut</td>\n",
       "      <td>0.023546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>risk_smoothed_weighted_lag2</td>\n",
       "      <td>0.022424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>opt_vs_caut_weighted_lag2</td>\n",
       "      <td>0.022305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>risk_smoothed_weighted_lag1</td>\n",
       "      <td>0.022173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8k_mean_sent</td>\n",
       "      <td>0.022089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8k_total_wscore_lag2</td>\n",
       "      <td>0.020986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>opt_vs_caut_weighted_lag1</td>\n",
       "      <td>0.020166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>opt_vs_caut_lag2</td>\n",
       "      <td>0.019704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Volume_lag1</td>\n",
       "      <td>0.019388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature  importance\n",
       "0      smoothed_news_sentiment    0.023619\n",
       "1                  opt_vs_caut    0.023546\n",
       "2  risk_smoothed_weighted_lag2    0.022424\n",
       "3    opt_vs_caut_weighted_lag2    0.022305\n",
       "4  risk_smoothed_weighted_lag1    0.022173\n",
       "5                 8k_mean_sent    0.022089\n",
       "6         8k_total_wscore_lag2    0.020986\n",
       "7    opt_vs_caut_weighted_lag1    0.020166\n",
       "8             opt_vs_caut_lag2    0.019704\n",
       "9                  Volume_lag1    0.019388"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Quick feature importance from a RandomForest\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X, y)\n",
    "fi = pd.Series(rf.feature_importances_, index=final_features).sort_values(ascending=False)\n",
    "fi_df = fi.reset_index()\n",
    "fi_df.columns = ['feature', 'importance']\n",
    "display(fi_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e0ffa-350a-47fa-b79f-47408a0782a7",
   "metadata": {},
   "source": [
    "##### The forest leans on price-based variables\n",
    "Your top-10 importances are dominated by lagged count_8k, volume (and its lag), the “opt_vs_caut” ratio (and its lag), plus a handful of sentiment stats. In effect, the model is relying more on sheer article counts and trading volume than on semantic content (for example, sent_8k_mean appears lower than count_8k_lag1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed881fd-0209-4490-910c-ccb0912e45ba",
   "metadata": {},
   "source": [
    "# Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "4c0d0d8e-d148-4431-93b0-a59058b29634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "fdb5de23-6f03-467f-9907-9f661c39fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load merged dataset\n",
    "df = pd.read_csv(\"Data/merged_dataset_weighted.csv\", parse_dates=[\"Week\"])\n",
    "df = df.sort_values(\"Week\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "5562b248-6011-4b2d-91db-b4bd50f53c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weekly return and target\n",
    "df[\"Return\"] = df[\"Close\"].pct_change()\n",
    "df[\"Target\"] = np.where(df[\"Return\"] > 0.01, 1,\n",
    "                np.where(df[\"Return\"] < -0.01, -1, 0))\n",
    "df = df.dropna(subset=[\"Return\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "2c53fecb-caa4-4227-b5f3-6a063fd31636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define excluded columns and base features\n",
    "exclude = {\"Week\", \"Close\", \"Return\", \"Target\", 'Year', 'Adj Close', 'mean_news_sentiment', 'std_news_sentiment', '10q_mda_sent', '10q_risk_sent',\n",
    "       '10q_mda_sent_weighted', '10q_risk_sent_weighted' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "bd4c9399-1820-4283-b3f9-60d1862401be",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = [col for col in df.columns if col not in exclude]\n",
    "# Generate lag feature names\n",
    "def make_lags(cols):\n",
    "    return [f\"{c}_lag1\" for c in cols] + [f\"{c}_lag2\" for c in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "a35b360f-81f0-4d64-aab3-37e00f56da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features in dataframe\n",
    "for col in base_features:\n",
    "    df[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "    df[f\"{col}_lag2\"] = df[col].shift(2)\n",
    "df = df.dropna().reset_index(drop=True)  # drop rows missing lags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4112978a-8fb5-45f6-889d-5a1d7e9ec22f",
   "metadata": {},
   "source": [
    "['num_positive',\n",
    " 'num_negative',\n",
    " 'num_neutral',\n",
    " 'num_news_articles',\n",
    " 'low_coverage_week',\n",
    " 'smoothed_news_sentiment',\n",
    " '10k_mean_sent',\n",
    " '10k_count',\n",
    " '10k_total_wscore',\n",
    " 'count_10q',\n",
    " 'mda_smoothed',\n",
    " 'risk_smoothed',\n",
    " 'mda_smoothed_weighted',\n",
    " 'risk_smoothed_weighted',\n",
    " 'opt_vs_caut',\n",
    " 'opt_vs_caut_weighted',\n",
    " '8k_mean_sent',\n",
    " '8k_count',\n",
    " '8k_total_wscore',\n",
    " 'Open',\n",
    " 'High',\n",
    " 'Low',\n",
    " 'Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "da250593-2b53-4128-8b02-1f52cda62fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "price_feats = [c for c in base_features if c in [\"Open\", \"High\", \"Low\", \"Volume\"]]\n",
    "weighted_feats = [c for c in base_features if c not in ['10k_mean_sent', 'mda_smoothed', 'risk_smoothed', 'opt_vs_caut', '8k_mean_sent'] and c not in price_feats]\n",
    "sentiment_feats = [c for c in base_features if c not in ['10k_total_wscore', 'mda_smoothed_weighted', 'risk_smoothed_weighted', 'opt_vs_caut_weighted', '8k_total_wscore'] and c not in price_feats]\n",
    "feature_groups = {\n",
    "    \"Price/Volume\": price_feats + make_lags(price_feats),\n",
    "    \"Weighted/Filings\": weighted_feats + make_lags(weighted_feats),\n",
    "    \"Sentiment/Filings\": sentiment_feats + make_lags(sentiment_feats),\n",
    "    \"All Features\": base_features + make_lags(base_features)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "049e951b-761a-458a-8f93-68358ba776b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df  # we'll subset by columns later\n",
    "y = df[\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "a774863d-0bcd-4659-8aa8-a1c22c45d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesSplit setup\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "e7efa22c-46d3-4f82-926a-6498b913ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation loops\n",
    "for name, feats in feature_groups.items():\n",
    "    feats = [c for c in feats if c in df.columns]  # ensure correct\n",
    "    acc_scores, f1_scores = [], []\n",
    "    for train_idx, test_idx in tscv.split(df):\n",
    "        X_train, X_test = df.iloc[train_idx][feats], df.iloc[test_idx][feats]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        acc_scores.append(accuracy_score(y_test, preds))\n",
    "        f1_scores.append(f1_score(y_test, preds, average=\"macro\"))\n",
    "    results.append({\n",
    "        \"Feature Group\": name,\n",
    "        \"Mean Accuracy\": np.mean(acc_scores),\n",
    "        \"Std Accuracy\": np.std(acc_scores),\n",
    "        \"Mean Macro-F1\": np.mean(f1_scores),\n",
    "        \"Std Macro-F1\": np.std(f1_scores)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "d82c24a4-17ee-45c5-acd6-d61a9333a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "49779f38-4004-410b-8890-9972280bf7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Group</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Std Accuracy</th>\n",
       "      <th>Mean Macro-F1</th>\n",
       "      <th>Std Macro-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Price/Volume</td>\n",
       "      <td>0.41250</td>\n",
       "      <td>0.060596</td>\n",
       "      <td>0.338378</td>\n",
       "      <td>0.087629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weighted/Filings</td>\n",
       "      <td>0.31875</td>\n",
       "      <td>0.060596</td>\n",
       "      <td>0.266839</td>\n",
       "      <td>0.043807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentiment/Filings</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.113537</td>\n",
       "      <td>0.320375</td>\n",
       "      <td>0.105249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All Features</td>\n",
       "      <td>0.36250</td>\n",
       "      <td>0.112847</td>\n",
       "      <td>0.288176</td>\n",
       "      <td>0.083341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Feature Group  Mean Accuracy  Std Accuracy  Mean Macro-F1  Std Macro-F1\n",
       "0       Price/Volume        0.41250      0.060596       0.338378      0.087629\n",
       "1   Weighted/Filings        0.31875      0.060596       0.266839      0.043807\n",
       "2  Sentiment/Filings        0.37500      0.113537       0.320375      0.105249\n",
       "3       All Features        0.36250      0.112847       0.288176      0.083341"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8105a7-bf00-4efd-a2a5-9b013a51fddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fusionnet)",
   "language": "python",
   "name": "fusionnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
