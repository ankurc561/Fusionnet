{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a435b903-9aee-4a14-b850-d8f33472ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 4 — Unweighted ablations: LR and HGB across 5 feature sets\n",
    "# Outputs are saved under Data/10_day_run/day4_*\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "522daaf5-ceed-48b6-89d1-acd6034bf019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load unweighted dataset ----------\n",
    "df = pd.read_csv(\"Data/10_day_run/merged_dataset.csv\")\n",
    "df[\"Week\"] = pd.to_datetime(df[\"Week\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"Week\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00eb7471-5f95-4ba6-8ae3-45177024fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Target: ±1% weekly move ----------\n",
    "df[\"Return_protocol\"] = df[\"Close\"].pct_change()\n",
    "delta = 0.01\n",
    "def to_label(r):\n",
    "    if pd.isna(r): return np.nan\n",
    "    if r >  delta: return  1\n",
    "    if r < -delta: return -1\n",
    "    return 0\n",
    "\n",
    "df[\"Target_protocol\"] = df[\"Return_protocol\"].apply(to_label)\n",
    "df = df.dropna(subset=[\"Target_protocol\"]).reset_index(drop=True)\n",
    "y = df[\"Target_protocol\"].astype(int).copy()\n",
    "labels_order = [-1, 0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c877913c-4c37-4f73-8109-7051e229223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Rolling-origin expanding splits (same as Day 3) ----------\n",
    "N = len(df)\n",
    "initial_train_weeks = max(52, int(0.5 * N))\n",
    "test_weeks         = max(16, int(0.1 * N))\n",
    "\n",
    "splits = []\n",
    "train_end = initial_train_weeks - 1\n",
    "sid = 1\n",
    "while True:\n",
    "    te_start = train_end + 1\n",
    "    te_end   = te_start + test_weeks - 1\n",
    "    if te_start >= N: break\n",
    "    if te_end >= N: te_end = N - 1\n",
    "    splits.append((sid, 0, int(train_end), int(te_start), int(te_end)))\n",
    "    sid += 1\n",
    "    if te_end >= N - 1: break\n",
    "    train_end = te_end\n",
    "\n",
    "pd.DataFrame(splits, columns=[\"split_id\",\"train_start\",\"train_end\",\"test_start\",\"test_end\"])\\\n",
    "  .to_csv(\"Data/10_day_run/day4_splits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d967889d-dd81-4ee2-9bf5-53169a84e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Unweighted feature families ----------\n",
    "news_feats = [c for c in [\n",
    "    \"mean_news_sentiment\",\"smoothed_news_sentiment\",\"num_news_articles\",\"low_coverage_week\"\n",
    "] if c in df.columns]\n",
    "\n",
    "filings_feats = [c for c in [\n",
    "    \"sent_10k_mean\",\"10q_mda_sent\",\"10q_risk_sent\",\"opt_vs_caut\",\n",
    "    \"sent_8k_mean\",\"count_10k\",\"count_10q\",\"count_8k\"\n",
    "] if c in df.columns]\n",
    "\n",
    "market_feats = [c for c in [\"Volume\"] if c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28962558-ae73-41c4-a047-ef7c47b99b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Lag builder ----------\n",
    "def add_lags(df, cols, lags=(1,2)):\n",
    "    out = df.copy()\n",
    "    for L in lags:\n",
    "        for c in cols:\n",
    "            if c in out.columns:\n",
    "                out[f\"{c}_lag{L}\"] = out[c].shift(L)\n",
    "    return out\n",
    "\n",
    "df_feats = df.copy()\n",
    "for fam in [news_feats, filings_feats, market_feats]:\n",
    "    df_feats = add_lags(df_feats, fam, lags=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed32412-e4ca-427c-899d-d62a73c8850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Design matrices for each feature set ----------\n",
    "def build_X(df, base_cols):\n",
    "    cols = []\n",
    "    for c in base_cols:\n",
    "        if c in df.columns:\n",
    "            cols.append(c)\n",
    "            for L in (1,2):\n",
    "                cl = f\"{c}_lag{L}\"\n",
    "                if cl in df.columns: cols.append(cl)\n",
    "    cols = list(dict.fromkeys(cols))  # de-dup\n",
    "    return df[cols].copy(), cols\n",
    "\n",
    "X_news,   cols_news   = build_X(df_feats, news_feats)\n",
    "X_filings,cols_filings= build_X(df_feats, filings_feats)\n",
    "X_market, cols_market = build_X(df_feats, market_feats)\n",
    "X_nf,     cols_nf     = build_X(df_feats, news_feats + filings_feats)\n",
    "X_nfv,    cols_nfv    = build_X(df_feats, news_feats + filings_feats + market_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b370bb95-2d37-4fab-911f-24ef5fca04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    \"news_only\": X_news,\n",
    "    \"filings_only\": X_filings,\n",
    "    \"market_only\": X_market,\n",
    "    \"news_plus_filings\": X_nf,\n",
    "    \"news_filings_volume\": X_nfv,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15f9d834-962a-4c7f-8355-0fea4ae0a7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 239 Splits: 6\n",
      "news_only              -> 12 features\n",
      "filings_only           -> 24 features\n",
      "market_only            -> 3 features\n",
      "news_plus_filings      -> 36 features\n",
      "news_filings_volume    -> 39 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Rows:\", N, \"Splits:\", len(splits))\n",
    "for name, X in feature_sets.items():\n",
    "    print(f\"{name:<22} -> {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a424e-c2f2-41d2-9c2b-3a77a7ef1c40",
   "metadata": {},
   "source": [
    "## Metrics, bootstrap CIs, paired tests vs Random-Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93f4d44e-a873-4919-8ccc-a01abe319233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Metrics, bootstrap, significance vs RW ----------\n",
    "\n",
    "from math import fabs\n",
    "from scipy.stats import chi2, norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a97a3a74-2f99-43ae-aa44-14324ad66903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc_scorer():\n",
    "    return make_scorer(matthews_corrcoef)\n",
    "\n",
    "def _onehot_matrix(y_true, classes):\n",
    "    \"\"\"Version-safe one-hot (handles both sparse_output and sparse kw).\"\"\"\n",
    "    try:\n",
    "        oh = OneHotEncoder(categories=[classes], sparse_output=False, handle_unknown=\"ignore\")\n",
    "    except TypeError:\n",
    "        oh = OneHotEncoder(categories=[classes], sparse=False, handle_unknown=\"ignore\")\n",
    "    return oh.fit_transform(y_true.reshape(-1, 1))\n",
    "\n",
    "def brier_multiclass(y_true, proba, classes):\n",
    "    Y = _onehot_matrix(y_true, classes)\n",
    "    return np.mean(np.sum((proba - Y)**2, axis=1))\n",
    "\n",
    "def reorder_proba(proba, est_classes, target_order=(-1, 0, 1)):\n",
    "    \"\"\"Reorder proba columns to match target_order.\"\"\"\n",
    "    idx = [list(est_classes).index(c) for c in target_order]\n",
    "    return proba[:, idx]\n",
    "\n",
    "def metric_tuple(y_true, y_pred, y_proba, classes):\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    mcc  = matthews_corrcoef(y_true, y_pred)\n",
    "    f1w  = f1_score(y_true, y_pred, average=\"weighted\", labels=classes, zero_division=0)\n",
    "    f1m  = f1_score(y_true, y_pred, average=\"macro\",   labels=classes, zero_division=0)\n",
    "    bri  = brier_multiclass(y_true, y_proba, classes) if y_proba is not None else np.nan\n",
    "    return acc, f1w, f1m, mcc, bri\n",
    "\n",
    "def mbb_indices(n, block_len=10, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    k = int(np.ceil(n / block_len))\n",
    "    starts = rng.integers(low=0, high=n, size=k)\n",
    "    idx = []\n",
    "    for s in starts:\n",
    "        idx.extend([(s+i) % n for i in range(block_len)])\n",
    "    return np.array(idx[:n], dtype=int)\n",
    "\n",
    "def pooled_with_ci(preds_df, block_len=10, B=2000, seed=42):\n",
    "    y_true = preds_df[\"y_true\"].astype(int).to_numpy()\n",
    "    y_pred = preds_df[\"y_pred\"].astype(int).to_numpy()\n",
    "    proba_cols = [c for c in preds_df.columns if c.startswith(\"p_\")]\n",
    "    proba = preds_df[proba_cols].to_numpy() if len(proba_cols)==3 else None\n",
    "    n = len(y_true)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    boot = []\n",
    "    for _ in range(B):\n",
    "        idx = mbb_indices(n, block_len=block_len, rng=rng)\n",
    "        boot.append(metric_tuple(y_true[idx], y_pred[idx], None if proba is None else proba[idx], labels_order))\n",
    "    boot = np.array(boot)\n",
    "    pt = np.array(metric_tuple(y_true, y_pred, proba, labels_order))\n",
    "    lo = np.percentile(boot, 2.5, axis=0); hi = np.percentile(boot, 97.5, axis=0)\n",
    "    return pd.DataFrame({\n",
    "        \"Metric\": [\"Accuracy\",\"Weighted F1\",\"Macro F1\",\"MCC\",\"Brier\"],\n",
    "        \"Point\": pt, \"CI Lower (95%)\": lo, \"CI Upper (95%)\": hi, \"n_weeks\": [n]*5\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a92aff1a-4dce-4212-869f-9454c4b457f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random-Walk predictions (Day 1)\n",
    "rw = pd.read_csv(\"Data/10_day_run/rw_predictions_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e89e11f-9085-4ff6-bc35-d26d93201dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_vs_rw(model_preds_csv):\n",
    "    model_df = pd.read_csv(model_preds_csv)\n",
    "    merged = pd.merge(\n",
    "        rw.rename(columns={\"y_pred\":\"y_pred_rw\"}),\n",
    "        model_df[[\"Week\",\"split_id\",\"y_pred\",\"y_true\"]],\n",
    "        on=[\"Week\",\"split_id\",\"y_true\"], how=\"inner\"\n",
    "    ).rename(columns={\"y_pred\":\"y_pred_model\"})\n",
    "    y_true = merged[\"y_true\"].astype(int).to_numpy()\n",
    "    y_rw   = merged[\"y_pred_rw\"].astype(int).to_numpy()\n",
    "    y_m    = merged[\"y_pred_model\"].astype(int).to_numpy()\n",
    "\n",
    "    hit_rw = (y_rw == y_true).astype(int)\n",
    "    hit_m  = (y_m  == y_true).astype(int)\n",
    "    n01 = int(((hit_rw == 0) & (hit_m == 1)).sum())  # RW wrong, model right\n",
    "    n10 = int(((hit_rw == 1) & (hit_m == 0)).sum())  # RW right, model wrong\n",
    "    if (n01 + n10) == 0:\n",
    "        mcnemar = {\"n01\": n01, \"n10\": n10, \"chi2\": 0.0, \"p_value\": 1.0}\n",
    "    else:\n",
    "        chi2_stat = (fabs(n01 - n10) - 1)**2 / (n01 + n10)\n",
    "        mcnemar = {\"n01\": n01, \"n10\": n10, \"chi2\": chi2_stat, \"p_value\": float(1 - chi2.cdf(chi2_stat, 1))}\n",
    "        \n",
    "    # Diebold–Mariano on 0–1 loss, h=1 → lag=0\n",
    "    loss_m  = (y_m  != y_true).astype(int)\n",
    "    loss_rw = (y_rw != y_true).astype(int)\n",
    "    d = loss_m - loss_rw\n",
    "    T = len(d); d_bar = d.mean()\n",
    "    var = d.var(ddof=0) / T\n",
    "    dm_stat = d_bar / (np.sqrt(var) + 1e-12)\n",
    "    dm = {\"dm_stat\": float(dm_stat), \"p_value\": float(2 * (1 - norm.cdf(abs(dm_stat)))), \"mean_diff\": float(d_bar), \"T\": int(T)}\n",
    "    return mcnemar, dm, len(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bcf1d8-d7dc-434c-aeb8-5af90f88419b",
   "metadata": {},
   "source": [
    "## Model trainers and ablation runner (LR + HGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a378209-5dcc-4f25-8612-5a6b07d3b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Trainers ----------\n",
    "def fit_predict_lr(X_tr, y_tr, X_te):\n",
    "    from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import matthews_corrcoef, make_scorer\n",
    "\n",
    "    def mcc_scorer(): \n",
    "        return make_scorer(matthews_corrcoef)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000, multi_class=\"multinomial\"))\n",
    "    ])\n",
    "    grid = {\"clf__C\": [0.3, 1.0, 3.0], \"clf__class_weight\": [None, \"balanced\"]}\n",
    "    gs = GridSearchCV(pipe, grid, scoring=mcc_scorer(), cv=TimeSeriesSplit(n_splits=3),\n",
    "                      n_jobs=-1, refit=True)\n",
    "    gs.fit(X_tr, y_tr)\n",
    "    best = gs.best_estimator_\n",
    "    raw_proba = best.predict_proba(X_te)\n",
    "    est_classes = best.named_steps[\"clf\"].classes_\n",
    "    proba = reorder_proba(raw_proba, est_classes, target_order=(-1,0,1))\n",
    "    y_hat = np.array([[-1,0,1][i] for i in proba.argmax(axis=1)])\n",
    "    return y_hat, proba, gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84d82819-f558-4292-a307-339e9a8f13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_hgb(X_tr, y_tr, X_te):\n",
    "    from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "    from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    from sklearn.metrics import matthews_corrcoef, make_scorer\n",
    "\n",
    "    def mcc_scorer(): \n",
    "        return make_scorer(matthews_corrcoef)\n",
    "\n",
    "    base = HistGradientBoostingClassifier(random_state=42)\n",
    "    grid = {\n",
    "        \"max_depth\": [3, None],\n",
    "        \"learning_rate\": [0.1],\n",
    "        \"max_iter\": [300],\n",
    "        \"min_samples_leaf\": [20],\n",
    "        \"l2_regularization\": [0.0],\n",
    "    }\n",
    "    gs = GridSearchCV(base, grid, scoring=mcc_scorer(), cv=TimeSeriesSplit(n_splits=3),\n",
    "                      n_jobs=-1, refit=True)\n",
    "    gs.fit(X_tr, y_tr)\n",
    "    best = gs.best_estimator_\n",
    "    calib = CalibratedClassifierCV(best, method=\"isotonic\", cv=TimeSeriesSplit(n_splits=3))\n",
    "    calib.fit(X_tr, y_tr)\n",
    "    raw_proba = calib.predict_proba(X_te)\n",
    "    est_classes = calib.classes_\n",
    "    proba = reorder_proba(raw_proba, est_classes, target_order=(-1,0,1))\n",
    "    y_hat = np.array([[-1,0,1][i] for i in proba.argmax(axis=1)])\n",
    "    return y_hat, proba, gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45b6a85f-a10c-44c3-856a-01637958df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Run ablation for a (model, feature_set) ----------\n",
    "def run_feature_ablation(model_name, trainer, feature_name, X):\n",
    "    results, preds_records, best_rows = [], [], []\n",
    "    for sid, tr0, tr1, te0, te1 in splits:\n",
    "        X_tr = X.iloc[tr0:tr1+1].copy(); y_tr = y.iloc[tr0:tr1+1].copy()\n",
    "        X_te = X.iloc[te0:te1+1].copy(); y_te = y.iloc[te0:te1+1].copy()\n",
    "\n",
    "        # Drop lag-induced NaNs\n",
    "        tr_mask = ~X_tr.isna().any(axis=1)\n",
    "        te_mask = ~X_te.isna().any(axis=1)\n",
    "        X_tr, y_tr = X_tr[tr_mask], y_tr[tr_mask]\n",
    "        X_te, y_te = X_te[te_mask], y_te[te_mask]\n",
    "        if len(X_tr) < 30 or len(X_te) < 5: \n",
    "            continue\n",
    "\n",
    "        y_hat, proba, best_params = trainer(X_tr, y_tr, X_te)\n",
    "        acc, f1w, f1m, mcc, bri = metric_tuple(y_te.values, y_hat, proba, labels_order)\n",
    "\n",
    "        results.append({\n",
    "            \"model\": model_name, \"feature_set\": feature_name, \"split_id\": sid, \"test_size\": len(y_te),\n",
    "            \"Accuracy\": acc, \"Weighted F1\": f1w, \"Macro F1\": f1m, \"MCC\": mcc, \"Brier\": bri\n",
    "        })\n",
    "\n",
    "        tmp = pd.DataFrame({\"Week\": df.loc[te0:te1, \"Week\"].values[te_mask.values],\n",
    "                            \"split_id\": sid, \"y_true\": y_te.values, \"y_pred\": y_hat})\n",
    "        for j, c in enumerate(labels_order):\n",
    "            tmp[f\"p_{c}\"] = proba[:, j]\n",
    "        preds_records.append(tmp)\n",
    "\n",
    "        best_rows.append({\"model\": model_name, \"feature_set\": feature_name, \"split_id\": sid, **best_params})\n",
    "\n",
    "    mdf = pd.DataFrame(results)\n",
    "    pdf = pd.concat(preds_records, ignore_index=True) if preds_records else pd.DataFrame()\n",
    "    bdf = pd.DataFrame(best_rows)\n",
    "\n",
    "    base = f\"Data/10_day_run/day4_{model_name}_{feature_name}\"\n",
    "    mdf.to_csv(base + \"_per_split.csv\", index=False)\n",
    "    pdf.to_csv(base + \"_preds.csv\", index=False)\n",
    "    bdf.to_csv(base + \"_best_params.csv\", index=False)\n",
    "\n",
    "    # Pooled CIs + significance vs RW\n",
    "    if not pdf.empty:\n",
    "        pooled = pooled_with_ci(pdf, block_len=10, B=2000, seed=42)\n",
    "        pooled.insert(0, \"model\", model_name)\n",
    "        pooled.insert(1, \"feature_set\", feature_name)\n",
    "        pooled.to_csv(base + \"_pooled_ci.csv\", index=False)\n",
    "\n",
    "        mc, dm, n_aligned = eval_vs_rw(base + \"_preds.csv\")\n",
    "    else:\n",
    "        pooled = pd.DataFrame(); mc = {}; dm = {}; n_aligned = 0\n",
    "\n",
    "    return mdf, pdf, bdf, pooled, mc, dm, n_aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c330e4db-4ade-457d-a52e-c14facfb7212",
   "metadata": {},
   "source": [
    "## Run the ablations and produce compact summary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "355da3a4-55b9-47f7-88aa-f30706ed3e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Run: LR and HGB × 5 feature sets ----------\n",
    "feature_mats = {\n",
    "    \"news_only\": X_news,\n",
    "    \"filings_only\": X_filings,\n",
    "    \"market_only\": X_market,\n",
    "    \"news_plus_filings\": X_nf,\n",
    "    \"news_filings_volume\": X_nfv,\n",
    "}\n",
    "\n",
    "summary_rows = []\n",
    "mcc_rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8876b3e-70fe-49d4-8f89-5ce9418680e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, trainer in [\n",
    "    (\"LR\",  fit_predict_lr),\n",
    "    (\"HGB\", fit_predict_hgb),\n",
    "]:\n",
    "    for feat_name, X in feature_mats.items():\n",
    "        print(f\"Running {model_name} × {feat_name} ...\")\n",
    "        mdf, pdf, bdf, pooled, mc, dm, n = run_feature_ablation(model_name, trainer, feat_name, X)\n",
    "        # Significance summary\n",
    "        summary_rows.append({\n",
    "            \"model\": model_name, \"feature_set\": feat_name, \"aligned_weeks\": n,\n",
    "            \"McNemar_n01\": mc.get(\"n01\", np.nan), \"McNemar_n10\": mc.get(\"n10\", np.nan),\n",
    "            \"McNemar_p\": mc.get(\"p_value\", np.nan),\n",
    "            \"DM_stat\": dm.get(\"dm_stat\", np.nan), \"DM_p\": dm.get(\"p_value\", np.nan),\n",
    "            \"DM_mean_loss_diff\": dm.get(\"mean_diff\", np.nan)\n",
    "        })\n",
    "        # MCC compact table\n",
    "        if not pooled.empty:\n",
    "            m = pooled[pooled[\"Metric\"]==\"MCC\"].iloc[0]\n",
    "            mcc_rows.append({\n",
    "                \"model\": model_name, \"feature_set\": feat_name,\n",
    "                \"Point\": m[\"Point\"], \"CI Lower (95%)\": m[\"CI Lower (95%)\"], \"CI Upper (95%)\": m[\"CI Upper (95%)\"],\n",
    "                \"n_weeks\": m[\"n_weeks\"]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3bedf4a-015b-40fe-84e5-a87b64e63e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summary_rows)\n",
    "mcc_table  = pd.DataFrame(mcc_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71fab81c-e2d3-4d56-8428-ba0b8d5db9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "Data/10_day_run/day4_summary_significance.csv\n",
      "Data/10_day_run/day4_mcc_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "summary_df.to_csv(\"Data/10_day_run/day4_summary_significance.csv\", index=False)\n",
    "mcc_table.to_csv(\"Data/10_day_run/day4_mcc_comparison.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"Data/10_day_run/day4_summary_significance.csv\")\n",
    "print(\"Data/10_day_run/day4_mcc_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f442f1d-0f23-40d3-812b-0171f291f6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>feature_set</th>\n",
       "      <th>aligned_weeks</th>\n",
       "      <th>McNemar_n01</th>\n",
       "      <th>McNemar_n10</th>\n",
       "      <th>McNemar_p</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p</th>\n",
       "      <th>DM_mean_loss_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>news_only</td>\n",
       "      <td>120</td>\n",
       "      <td>25</td>\n",
       "      <td>32</td>\n",
       "      <td>0.426777</td>\n",
       "      <td>0.930512</td>\n",
       "      <td>0.352106</td>\n",
       "      <td>0.058333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>filings_only</td>\n",
       "      <td>120</td>\n",
       "      <td>29</td>\n",
       "      <td>35</td>\n",
       "      <td>0.531971</td>\n",
       "      <td>0.751764</td>\n",
       "      <td>0.452193</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>market_only</td>\n",
       "      <td>120</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>0.596242</td>\n",
       "      <td>-0.663480</td>\n",
       "      <td>0.507023</td>\n",
       "      <td>-0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR</td>\n",
       "      <td>news_plus_filings</td>\n",
       "      <td>120</td>\n",
       "      <td>28</td>\n",
       "      <td>36</td>\n",
       "      <td>0.381574</td>\n",
       "      <td>1.004193</td>\n",
       "      <td>0.315286</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR</td>\n",
       "      <td>news_filings_volume</td>\n",
       "      <td>120</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>0.608548</td>\n",
       "      <td>0.641280</td>\n",
       "      <td>0.521341</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HGB</td>\n",
       "      <td>news_only</td>\n",
       "      <td>120</td>\n",
       "      <td>26</td>\n",
       "      <td>34</td>\n",
       "      <td>0.366157</td>\n",
       "      <td>1.037417</td>\n",
       "      <td>0.299542</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HGB</td>\n",
       "      <td>filings_only</td>\n",
       "      <td>120</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>0.185326</td>\n",
       "      <td>1.470046</td>\n",
       "      <td>0.141549</td>\n",
       "      <td>0.091667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HGB</td>\n",
       "      <td>market_only</td>\n",
       "      <td>120</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "      <td>0.200415</td>\n",
       "      <td>1.420193</td>\n",
       "      <td>0.155552</td>\n",
       "      <td>0.091667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HGB</td>\n",
       "      <td>news_plus_filings</td>\n",
       "      <td>120</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.374003</td>\n",
       "      <td>1.020399</td>\n",
       "      <td>0.307539</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HGB</td>\n",
       "      <td>news_filings_volume</td>\n",
       "      <td>120</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>0.418492</td>\n",
       "      <td>0.947403</td>\n",
       "      <td>0.343433</td>\n",
       "      <td>0.058333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model          feature_set  aligned_weeks  McNemar_n01  McNemar_n10  \\\n",
       "0    LR            news_only            120           25           32   \n",
       "1    LR         filings_only            120           29           35   \n",
       "2    LR          market_only            120           31           26   \n",
       "3    LR    news_plus_filings            120           28           36   \n",
       "4    LR  news_filings_volume            120           28           33   \n",
       "5   HGB            news_only            120           26           34   \n",
       "6   HGB         filings_only            120           23           34   \n",
       "7   HGB          market_only            120           25           36   \n",
       "8   HGB    news_plus_filings            120           27           35   \n",
       "9   HGB  news_filings_volume            120           24           31   \n",
       "\n",
       "   McNemar_p   DM_stat      DM_p  DM_mean_loss_diff  \n",
       "0   0.426777  0.930512  0.352106           0.058333  \n",
       "1   0.531971  0.751764  0.452193           0.050000  \n",
       "2   0.596242 -0.663480  0.507023          -0.041667  \n",
       "3   0.381574  1.004193  0.315286           0.066667  \n",
       "4   0.608548  0.641280  0.521341           0.041667  \n",
       "5   0.366157  1.037417  0.299542           0.066667  \n",
       "6   0.185326  1.470046  0.141549           0.091667  \n",
       "7   0.200415  1.420193  0.155552           0.091667  \n",
       "8   0.374003  1.020399  0.307539           0.066667  \n",
       "9   0.418492  0.947403  0.343433           0.058333  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d53f536b-8d3d-4f28-ad70-6d08341bfac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>feature_set</th>\n",
       "      <th>Point</th>\n",
       "      <th>CI Lower (95%)</th>\n",
       "      <th>CI Upper (95%)</th>\n",
       "      <th>n_weeks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>news_only</td>\n",
       "      <td>-0.043054</td>\n",
       "      <td>-0.173905</td>\n",
       "      <td>0.094212</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>filings_only</td>\n",
       "      <td>-0.023820</td>\n",
       "      <td>-0.145022</td>\n",
       "      <td>0.102527</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>market_only</td>\n",
       "      <td>0.080077</td>\n",
       "      <td>-0.044820</td>\n",
       "      <td>0.192079</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR</td>\n",
       "      <td>news_plus_filings</td>\n",
       "      <td>-0.055214</td>\n",
       "      <td>-0.153734</td>\n",
       "      <td>0.040854</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR</td>\n",
       "      <td>news_filings_volume</td>\n",
       "      <td>-0.029851</td>\n",
       "      <td>-0.133759</td>\n",
       "      <td>0.078052</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HGB</td>\n",
       "      <td>news_only</td>\n",
       "      <td>-0.085468</td>\n",
       "      <td>-0.195050</td>\n",
       "      <td>0.027178</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HGB</td>\n",
       "      <td>filings_only</td>\n",
       "      <td>-0.149956</td>\n",
       "      <td>-0.251317</td>\n",
       "      <td>-0.034757</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HGB</td>\n",
       "      <td>market_only</td>\n",
       "      <td>-0.115130</td>\n",
       "      <td>-0.233177</td>\n",
       "      <td>0.027814</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HGB</td>\n",
       "      <td>news_plus_filings</td>\n",
       "      <td>-0.127649</td>\n",
       "      <td>-0.209639</td>\n",
       "      <td>-0.024097</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HGB</td>\n",
       "      <td>news_filings_volume</td>\n",
       "      <td>-0.099405</td>\n",
       "      <td>-0.216193</td>\n",
       "      <td>0.018686</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model          feature_set     Point  CI Lower (95%)  CI Upper (95%)  \\\n",
       "0    LR            news_only -0.043054       -0.173905        0.094212   \n",
       "1    LR         filings_only -0.023820       -0.145022        0.102527   \n",
       "2    LR          market_only  0.080077       -0.044820        0.192079   \n",
       "3    LR    news_plus_filings -0.055214       -0.153734        0.040854   \n",
       "4    LR  news_filings_volume -0.029851       -0.133759        0.078052   \n",
       "5   HGB            news_only -0.085468       -0.195050        0.027178   \n",
       "6   HGB         filings_only -0.149956       -0.251317       -0.034757   \n",
       "7   HGB          market_only -0.115130       -0.233177        0.027814   \n",
       "8   HGB    news_plus_filings -0.127649       -0.209639       -0.024097   \n",
       "9   HGB  news_filings_volume -0.099405       -0.216193        0.018686   \n",
       "\n",
       "   n_weeks  \n",
       "0      120  \n",
       "1      120  \n",
       "2      120  \n",
       "3      120  \n",
       "4      120  \n",
       "5      120  \n",
       "6      120  \n",
       "7      120  \n",
       "8      120  \n",
       "9      120  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcc_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e1cbd-388f-4435-8700-862231c509a0",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "660d3b15-0847-45ff-929b-46a52fd52eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR_market_only': {'pooled': {'Accuracy': 0.4166666666666667,\n",
       "   'Weighted F1': 0.37254765984063526,\n",
       "   'Macro F1': 0.33772266188028616,\n",
       "   'MCC': 0.0800769866871827,\n",
       "   'n': 120.0},\n",
       "  'n_aligned': 120,\n",
       "  'RW_Accuracy': 0.375,\n",
       "  'RW_WeightedF1': 0.37526197364191577,\n",
       "  'RW_MacroF1': 0.37487022712817253,\n",
       "  'RW_MCC': np.float64(0.04812781537265903)},\n",
       " 'LR_news_filings_volume': {'pooled': {'Accuracy': 0.3333333333333333,\n",
       "   'Weighted F1': 0.33045045514557714,\n",
       "   'Macro F1': 0.3153243925601649,\n",
       "   'MCC': -0.02985085940311298,\n",
       "   'n': 120.0},\n",
       "  'n_aligned': 120,\n",
       "  'RW_Accuracy': 0.375,\n",
       "  'RW_WeightedF1': 0.37526197364191577,\n",
       "  'RW_MacroF1': 0.37487022712817253,\n",
       "  'RW_MCC': np.float64(0.04812781537265903)}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Day 4 — Deep-dive on the two selected ablation rows (LR × market_only vs LR × news_filings_volume)\n",
    "# We'll compute confusion matrices, align to Random-Walk, and build simple reliability tables.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, matthews_corrcoef\n",
    "\n",
    "# Load inputs\n",
    "mkt_pooled  = pd.read_csv(\"Data/10_day_run/day4_LR_market_only_pooled_ci.csv\")\n",
    "nfv_pooled  = pd.read_csv(\"Data/10_day_run/day4_LR_news_filings_volume_pooled_ci.csv\")\n",
    "mkt_preds   = pd.read_csv(\"Data/10_day_run/day4_LR_market_only_preds.csv\")\n",
    "nfv_preds   = pd.read_csv(\"Data/10_day_run/day4_LR_news_filings_volume_preds.csv\")\n",
    "\n",
    "rw = pd.read_csv(\"Data/10_day_run/rw_predictions_all.csv\")\n",
    "\n",
    "# Ensure consistent dtypes\n",
    "for df in (mkt_preds, nfv_preds, rw):\n",
    "    df[\"Week\"] = pd.to_datetime(df[\"Week\"], errors=\"coerce\")\n",
    "    if \"y_true\" in df.columns:\n",
    "        df[\"y_true\"] = df[\"y_true\"].astype(int)\n",
    "    if \"y_pred\" in df.columns:\n",
    "        df[\"y_pred\"] = df[\"y_pred\"].astype(int)\n",
    "\n",
    "# Helper: pooled metrics from preds (sanity check)\n",
    "labels_order = [-1,0,1]\n",
    "def pooled_metrics_from_preds(df):\n",
    "    y_true = df[\"y_true\"].to_numpy()\n",
    "    y_pred = df[\"y_pred\"].to_numpy()\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    mcc  = matthews_corrcoef(y_true, y_pred)\n",
    "    f1w  = f1_score(y_true, y_pred, average=\"weighted\", labels=labels_order, zero_division=0)\n",
    "    f1m  = f1_score(y_true, y_pred, average=\"macro\", labels=labels_order, zero_division=0)\n",
    "    return pd.Series({\"Accuracy\":acc,\"Weighted F1\":f1w,\"Macro F1\":f1m,\"MCC\":mcc,\"n\":len(df)})\n",
    "\n",
    "mkt_pm = pooled_metrics_from_preds(mkt_preds)\n",
    "nfv_pm = pooled_metrics_from_preds(nfv_preds)\n",
    "\n",
    "# Confusion matrices\n",
    "def cm_df(df, labels=(-1,0,1)):\n",
    "    cm = confusion_matrix(df[\"y_true\"], df[\"y_pred\"], labels=labels)\n",
    "    return pd.DataFrame(cm, index=[f\"true_{l}\" for l in labels], columns=[f\"pred_{l}\" for l in labels])\n",
    "\n",
    "mkt_cm = cm_df(mkt_preds)\n",
    "nfv_cm = cm_df(nfv_preds)\n",
    "\n",
    "# Align each model to RW on the exact same weeks and compute RW metrics\n",
    "def aligned_rw_metrics(model_preds):\n",
    "    merged = pd.merge(\n",
    "        rw.rename(columns={\"y_pred\":\"y_pred_rw\"}),\n",
    "        model_preds[[\"Week\",\"split_id\",\"y_true\"]],\n",
    "        on=[\"Week\",\"split_id\",\"y_true\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    y_true = merged[\"y_true\"].astype(int).to_numpy()\n",
    "    y_rw   = merged[\"y_pred_rw\"].astype(int).to_numpy()\n",
    "    return {\n",
    "        \"n_aligned\": len(merged),\n",
    "        \"RW_Accuracy\": accuracy_score(y_true, y_rw),\n",
    "        \"RW_WeightedF1\": f1_score(y_true, y_rw, average=\"weighted\", labels=labels_order, zero_division=0),\n",
    "        \"RW_MacroF1\": f1_score(y_true, y_rw, average=\"macro\", labels=labels_order, zero_division=0),\n",
    "        \"RW_MCC\": matthews_corrcoef(y_true, y_rw),\n",
    "    }\n",
    "\n",
    "mkt_rw = aligned_rw_metrics(mkt_preds)\n",
    "nfv_rw = aligned_rw_metrics(nfv_preds)\n",
    "\n",
    "# Reliability tables: decile bins for each class's predicted probability\n",
    "def reliability_table(df, cls, n_bins=10):\n",
    "    p = df[[f\"p_{c}\" for c in (-1,0,1)]].to_numpy()[:, [(-1,0,1).index(cls)][0]]\n",
    "    y = (df[\"y_true\"].to_numpy() == cls).astype(int)\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    mids = 0.5*(bins[:-1]+bins[1:])\n",
    "    idx = np.digitize(p, bins)-1\n",
    "    rows = []\n",
    "    for b in range(n_bins):\n",
    "        mask = idx==b\n",
    "        n = int(mask.sum())\n",
    "        if n==0:\n",
    "            rows.append({\"bin_mid\":mids[b],\"n\":0,\"mean_pred\":np.nan,\"emp_freq\":np.nan})\n",
    "        else:\n",
    "            rows.append({\n",
    "                \"bin_mid\": mids[b],\n",
    "                \"n\": n,\n",
    "                \"mean_pred\": float(p[mask].mean()),\n",
    "                \"emp_freq\": float(y[mask].mean())\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "mkt_rel_up   = reliability_table(mkt_preds, 1)\n",
    "mkt_rel_neu  = reliability_table(mkt_preds, 0)\n",
    "mkt_rel_down = reliability_table(mkt_preds, -1)\n",
    "\n",
    "nfv_rel_up   = reliability_table(nfv_preds, 1)\n",
    "nfv_rel_neu  = reliability_table(nfv_preds, 0)\n",
    "nfv_rel_down = reliability_table(nfv_preds, -1)\n",
    "\n",
    "# Package key summaries to display\n",
    "summary_rows = [\n",
    "    {\n",
    "        \"model\":\"LR\", \"feature_set\":\"market_only\",\n",
    "        **mkt_pm.to_dict(),\n",
    "        **{f\"RW_{k.split('_')[1]}\":v for k,v in mkt_rw.items() if k!=\"n_aligned\"},\n",
    "        \"aligned_weeks\": mkt_rw[\"n_aligned\"]\n",
    "    },\n",
    "    {\n",
    "        \"model\":\"LR\", \"feature_set\":\"news_filings_volume\",\n",
    "        **nfv_pm.to_dict(),\n",
    "        **{f\"RW_{k.split('_')[1]}\":v for k,v in nfv_rw.items() if k!=\"n_aligned\"},\n",
    "        \"aligned_weeks\": nfv_rw[\"n_aligned\"]\n",
    "    },\n",
    "]\n",
    "summary = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Save everything for the report\n",
    "mkt_cm.to_csv(\"Data/10_day_run/day4_lr_market_only_confusion_matrix.csv\", index=True)\n",
    "nfv_cm.to_csv(\"Data/10_day_run/day4_lr_news_filings_volume_confusion_matrix.csv\", index=True)\n",
    "\n",
    "mkt_rel_up.to_csv(\"Data/10_day_run/day4_lr_market_only_reliability_up.csv\", index=False)\n",
    "mkt_rel_neu.to_csv(\"Data/10_day_run/day4_lr_market_only_reliability_neutral.csv\", index=False)\n",
    "mkt_rel_down.to_csv(\"Data/10_day_run/day4_lr_market_only_reliability_down.csv\", index=False)\n",
    "\n",
    "nfv_rel_up.to_csv(\"Data/10_day_run/day4_lr_news_filings_volume_reliability_up.csv\", index=False)\n",
    "nfv_rel_neu.to_csv(\"Data/10_day_run/day4_lr_news_filings_volume_reliability_neutral.csv\", index=False)\n",
    "nfv_rel_down.to_csv(\"Data/10_day_run/day4_lr_news_filings_volume_reliability_down.csv\", index=False)\n",
    "\n",
    "# Return quick dict with core numbers so you can copy into Results\n",
    "{\n",
    " \"LR_market_only\": {\"pooled\": mkt_pm.to_dict(), **mkt_rw},\n",
    " \"LR_news_filings_volume\": {\"pooled\": nfv_pm.to_dict(), **nfv_rw}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52bd6c-cfe2-4c42-a8d4-aec49a829f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fusionnet)",
   "language": "python",
   "name": "fusionnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
